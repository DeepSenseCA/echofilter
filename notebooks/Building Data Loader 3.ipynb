{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import echofilter.shardloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_DIR = '/media/scott/scratch/Datasets/dsforce/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_pth = 'Survey17/Survey17_GR1_S3W_F'\n",
    "timestamps, depths, signals, d_top, d_bot = echofilter.shardloader.load_transect_from_shards_rel(\n",
    "    transect_pth, 100, 800,\n",
    "    root_data_dir=ROOT_DATA_DIR,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.pcolormesh(timestamps, -depths, signals.T)\n",
    "plt.plot(timestamps, -d_bot, 'b')\n",
    "plt.plot(timestamps, -d_top, 'c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_pth = 'Survey17/Survey17_GR1_S3W_F'\n",
    "timestamps, depths, signals, d_top, d_bot = echofilter.shardloader.load_transect_from_shards_rel(\n",
    "    transect_pth, -100, 800,\n",
    "    root_data_dir=ROOT_DATA_DIR,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.pcolormesh(timestamps, -depths, signals.T)\n",
    "plt.plot(timestamps, -d_bot, 'b')\n",
    "plt.plot(timestamps, -d_top, 'c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_pth = 'Survey17/Survey17_GR1_S3W_F'\n",
    "timestamps, depths, signals, d_top, d_bot = echofilter.shardloader.load_transect_from_shards_rel(\n",
    "    transect_pth, 0, 128,\n",
    "    root_data_dir=ROOT_DATA_DIR,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.pcolormesh(timestamps, -depths, signals.T)\n",
    "plt.plot(timestamps, -d_bot, 'b')\n",
    "plt.plot(timestamps, -d_top, 'c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransectDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            transect_paths,\n",
    "            window_len=128,\n",
    "            crop_depth=70,\n",
    "            num_windows_per_transect=0,\n",
    "            use_dynamic_offsets=True,\n",
    "            transform_pre=None,\n",
    "            transform_post=None,\n",
    "            ):\n",
    "        '''\n",
    "        TransectDataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transect_paths : list\n",
    "            Absolute paths to transects.\n",
    "        window_len : int\n",
    "            Width (number of timestamps) to load. Default is `128`.\n",
    "        crop_depth : float\n",
    "            Maximum depth to include, in metres. Deeper data will be cropped away.\n",
    "            Default is `70`.\n",
    "        num_windows_per_transect : int\n",
    "            Number of windows to extract for each transect. Start indices for the\n",
    "            windows will be equally spaced across the total width of the transect.\n",
    "            If this is `0`, the number of windows will be inferred automatically\n",
    "            based on `window_len` and the total width of the transect, resulting\n",
    "            in a different number of windows for each transect. Default is `0`.\n",
    "        use_dynamic_offsets : bool\n",
    "            Whether starting indices for each window should be randomly offset.\n",
    "            Set to `True` for training and `False` for testing. Default is `True`.\n",
    "        transform_pre : callable\n",
    "            Operations to perform to the dictionary containing a single sample.\n",
    "            These are performed before generating the masks. Default is `None`.\n",
    "        transform_post : callable\n",
    "            Operations to perform to the dictionary containing a single sample.\n",
    "            These are performed after generating the masks. Default is `None`.\n",
    "        '''\n",
    "        super(TransectDataset, self).__init__()\n",
    "        self.window_len = window_len\n",
    "        self.crop_depth = crop_depth\n",
    "        self.num_windows = num_windows_per_transect\n",
    "        self.use_dynamic_offsets = use_dynamic_offsets\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "\n",
    "        self.datapoints = []\n",
    "\n",
    "        for transect_path in transect_paths:\n",
    "            # Lookup the number of rows in the transect\n",
    "            # Load the sharding metadata\n",
    "            with open(os.path.join(transect_path, 'shard_size.txt'), 'r') as f:\n",
    "                n_timestamps, shard_len = f.readline().strip().split(',')\n",
    "                n_timestamps = int(n_timestamps)\n",
    "            # Generate an array for window centers within the transect\n",
    "            # - if this is for training, we want to randomise the offsets\n",
    "            # - if this is for validation, we want stable windows\n",
    "            num_windows = self.num_windows\n",
    "            if self.num_windows is None or self.num_windows == 0:\n",
    "                # Load enough windows to include all datapoints\n",
    "                num_windows = int(np.ceil(n_timestamps / self.window_len))\n",
    "            centers = np.linspace(0, n_timestamps, num_windows + 1)[:num_windows]\n",
    "            if len(centers) > 1:\n",
    "                max_dy_offset = centers[1] - centers[0]\n",
    "            else:\n",
    "                max_dy_offset = n_timestamps\n",
    "            if self.use_dynamic_offsets:\n",
    "                centers += np.random.rand() * max_dy_offset\n",
    "            else:\n",
    "                centers += max_dy_offset / 2\n",
    "            centers = np.round(centers)\n",
    "            # Add each (transect, center) to the list for this epoch\n",
    "            for center_idx in centers:\n",
    "                self.datapoints.append((transect_path, int(center_idx)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        transect_pth, center_idx = self.datapoints[index]\n",
    "        # Load data from shards\n",
    "        timestamps, depths, signals, d_top, d_bot = echofilter.shardloader.load_transect_from_shards_abs(\n",
    "            transect_pth,\n",
    "            center_idx - int(self.window_len / 2),\n",
    "            center_idx - int(self.window_len / 2) + self.window_len,\n",
    "        )\n",
    "        sample = {\n",
    "            'timestamps': timestamps,\n",
    "            'depths': depths,\n",
    "            'signals': signals,\n",
    "            'd_top': d_top,\n",
    "            'd_bot': d_bot,\n",
    "        }\n",
    "        if self.transform_pre is not None:\n",
    "            sample = self.transform_pre(sample)\n",
    "        # Apply depth crop\n",
    "        depth_crop_mask = sample['depths'] <= self.crop_depth\n",
    "        sample['depths'] = sample['depths'][depth_crop_mask]\n",
    "        sample['signals'] = sample['signals'][:, depth_crop_mask]\n",
    "        # Convert lines to masks\n",
    "        ddepths = np.broadcast_to(sample['depths'], sample['signals'].shape)\n",
    "        mask_top = np.single(ddepths < np.expand_dims(sample['d_top'], -1))\n",
    "        mask_bot = np.single(ddepths > np.expand_dims(sample['d_bot'], -1))\n",
    "        sample['mask_top'] = mask_top\n",
    "        sample['mask_bot'] = mask_bot\n",
    "        sample['r_top'] = sample['d_top'] / abs(sample['depths'][-1] - sample['depths'][0])\n",
    "        sample['r_bot'] = sample['d_bot'] / abs(sample['depths'][-1] - sample['depths'][0])\n",
    "        if self.transform_post is not None:\n",
    "            sample = self.transform_post(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_paths = [os.path.join(ROOT_DATA_DIR, 'surveyExports_sharded/Survey17/Survey17_GR1_S3W_F')] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransectDataset(transect_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['signals'])\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_top'])\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_bot'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['signals'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in loader:\n",
    "    print(sample['signals'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    '''\n",
    "    Rescale the image(s) in a sample to a given size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size : tuple or int\n",
    "        Desired output size. If tuple, output is matched to output_size. If int,\n",
    "        output is square.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            output_size = (output_size, output_size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        for key in ('signals', 'mask_top', 'mask_bot'):\n",
    "            if key in sample:\n",
    "                sample[key] = skimage.transform.resize(\n",
    "                    sample[key],\n",
    "                    self.output_size,\n",
    "                    clip=False,\n",
    "                    preserve_range=False,\n",
    "                )\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    '''\n",
    "    Normalize mean and standard deviation of image.\n",
    "\n",
    "    Note that changes are made inplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float\n",
    "        Expected sample pixel mean.\n",
    "    stdev : float\n",
    "        Expected sample standard deviation of pixel intensities.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mean, stdev):\n",
    "        self.mean = mean\n",
    "        self.stdev = stdev\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        sample['signals'] -= self.mean\n",
    "        sample['signals'] /= self.stdev\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomReflection(object):\n",
    "    '''\n",
    "    Randomly reflect a sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : int, optional\n",
    "        Axis to reflect. Default is 0.\n",
    "    p : float, optional\n",
    "        Probability of reflection. Default is 0.5.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, axis=0, p=0.5):\n",
    "        self.axis = axis\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        if random.random() > self.p:\n",
    "            # Nothing to do\n",
    "            return sample\n",
    "        \n",
    "        # Reflect x co-ordinates\n",
    "        sample['timestamps'] = sample['timestamps'][::-1]\n",
    "\n",
    "        # Reflect data\n",
    "        for key in ('signals', 'd_top', 'd_bot', 'mask_top', 'mask_bot'):\n",
    "            if key in sample:\n",
    "                sample[key] = np.flip(sample[key], self.axis)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStretchDepth(object):\n",
    "    '''\n",
    "    Rescale a set of images in a sample to a given size.\n",
    "    \n",
    "    Note that this transform doesn't change images, just the `depth`, `d_top`, and `d_bot`.\n",
    "    Note that changes are made inplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_factor : float\n",
    "        Maximum stretch factor. A number between `[1, 1 + max_factor]` will be generated,\n",
    "        and the depth will either be divided or multiplied by the generated stretch\n",
    "        factor.\n",
    "    expected_bottom_gap : float\n",
    "        Expected gap between actual ocean floor and target bottom line.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_factor, expected_bottom_gap=1):\n",
    "        self.max_factor = max_factor\n",
    "        self.expected_bottom_gap = expected_bottom_gap\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        factor = random.uniform(1.0, 1.0 + self.max_factor)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            factor = 1. / factor\n",
    "\n",
    "        sample['d_bot'] += self.expected_bottom_gap\n",
    "        for key in ('depths', 'd_top', 'd_bot'):\n",
    "            sample[key] *= factor\n",
    "        sample['d_bot'] -= self.expected_bottom_gap\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropWidth(object):\n",
    "    '''\n",
    "    Randomly crop a sample in the width dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_crop_fraction : float\n",
    "        Maximum amount of material to crop away, as a fraction of the total width.\n",
    "        The `crop_fraction` will be sampled uniformly from the range\n",
    "        `[0, max_crop_fraction]`. The crop is always centred.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_crop_fraction):\n",
    "        self.max_crop_fraction = max_crop_fraction\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        width = sample['signals'].shape[0]\n",
    "\n",
    "        crop_fraction = random.uniform(0., self.max_crop_fraction)\n",
    "        crop_amount = crop_fraction * width\n",
    "        \n",
    "        lft = int(crop_amount / 2)\n",
    "        rgt = lft + width - int(crop_amount)\n",
    "\n",
    "        # Crop data\n",
    "        for key in ('timestamps', 'signals', 'd_top', 'd_bot', 'mask_top', 'mask_bot'):\n",
    "            if key in sample:\n",
    "                sample[key] = sample[key][lft:rgt]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorJitter(object):\n",
    "    '''\n",
    "    Randomly change the brightness and contrast of a normalized image.\n",
    "\n",
    "    Note that changes are made inplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    brightness : float or tuple of float (min, max)\n",
    "        How much to jitter brightness. `brightness_factor` is chosen uniformly from\n",
    "        `[-brightness, brightness]`\n",
    "        or the given `[min, max]`. `brightness_factor` is then added to the image.\n",
    "    contrast : (float or tuple of float (min, max))\n",
    "        How much to jitter contrast. `contrast_factor` is chosen uniformly from\n",
    "        `[max(0, 1 - contrast), 1 + contrast]`\n",
    "        or the given `[min, max]`. Should be non negative numbers.\n",
    "    '''\n",
    "    def __init__(self, brightness=0, contrast=0):\n",
    "        self.brightness = self._check_input(\n",
    "            brightness,\n",
    "            'brightness',\n",
    "            center=0,\n",
    "            bound=(float('-inf'), float('inf')),\n",
    "            clip_first_on_zero=False,\n",
    "        )\n",
    "        self.contrast = self._check_input(contrast, 'contrast')\n",
    "\n",
    "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
    "        if isinstance(value, (float, int)):\n",
    "            if value < 0:\n",
    "                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\"{} should be a single number or a list/tuple with length 2.\".format(name))\n",
    "\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        init_op = random.randint(0, 1)\n",
    "        for i_op in range(2):\n",
    "            op_num = (init_op + i_op) % 2\n",
    "            if op_num == 0 and self.brightness is not None:\n",
    "                brightness_factor = random.uniform(self.brightness[0], self.brightness[1])\n",
    "                sample['signals'] += brightness_factor\n",
    "            elif op_num == 1 and self.contrast is not None:\n",
    "                contrast_factor = random.uniform(self.contrast[0], self.contrast[1])\n",
    "                sample['signals'] *= contrast_factor\n",
    "        return sample\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += 'brightness={0}'.format(self.brightness)\n",
    "        format_string += ', contrast={0})'.format(self.contrast)\n",
    "        format_string += ')'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_pre = torchvision.transforms.Compose([\n",
    "    RandomCropWidth(0.5),\n",
    "    RandomStretchDepth(0.5),\n",
    "    RandomReflection(),\n",
    "])\n",
    "train_transform_post = torchvision.transforms.Compose([\n",
    "    Rescale((128, 512)),\n",
    "    Normalize(-70, 22),\n",
    "    ColorJitter(0.5, 0.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TransectDataset(\n",
    "    transect_paths,\n",
    "    window_len=192,\n",
    "    crop_depth=70,\n",
    "    num_windows_per_transect=10,\n",
    "    use_dynamic_offsets=True,\n",
    "    transform_pre=composed_pre,\n",
    "    transform_post=composed_post,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_train[0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.pcolormesh(\n",
    "    np.linspace(*sample['timestamps'][[0, -1]], sample['signals'].shape[0]),\n",
    "    -np.linspace(sample['depths'][0], sample['depths'][-1], sample['signals'].shape[1]),\n",
    "    sample['signals'].T\n",
    ")\n",
    "plt.plot(np.linspace(*sample['timestamps'][[0, -1]], sample['d_bot'].shape[0]), -sample['d_bot'], 'b')\n",
    "plt.plot(np.linspace(*sample['timestamps'][[0, -1]], sample['d_top'].shape[0]), -sample['d_top'], 'c')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['signals'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_top'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_bot'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['r_top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['r_bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = torchvision.transforms.Compose([\n",
    "    Rescale((128, 512)),\n",
    "    Normalize(-70, 22),\n",
    "])\n",
    "\n",
    "dataset_val = TransectDataset(\n",
    "    transect_paths,\n",
    "    window_len=128,\n",
    "    crop_depth=70,\n",
    "    num_windows_per_transect=20,\n",
    "    use_dynamic_offsets=False,\n",
    "    transform_post=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_val[0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.pcolormesh(\n",
    "    np.linspace(*sample['timestamps'][[0, -1]], sample['signals'].shape[0]),\n",
    "    -np.linspace(sample['depths'][0], sample['depths'][-1], sample['signals'].shape[1]),\n",
    "    sample['signals'].T\n",
    ")\n",
    "plt.plot(np.linspace(*sample['timestamps'][[0, -1]], sample['d_bot'].shape[0]), -sample['d_bot'], 'b')\n",
    "plt.plot(np.linspace(*sample['timestamps'][[0, -1]], sample['d_top'].shape[0]), -sample['d_top'], 'c')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['signals'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_top'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(sample['mask_bot'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val.datapoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
