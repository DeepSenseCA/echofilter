{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import echofilter.raw.shardloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT_DATA_DIR = \"/media/scott/scratch/Datasets/dsforce/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transect_pth = \"Survey17/Survey17_GR1_S3W_F\"\n",
        "(\n",
        "    timestamps,\n",
        "    depths,\n",
        "    signals,\n",
        "    d_top,\n",
        "    d_bot,\n",
        ") = echofilter.raw.shardloader.load_transect_from_shards_rel(\n",
        "    transect_pth,\n",
        "    100,\n",
        "    800,\n",
        "    root_data_dir=ROOT_DATA_DIR,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.pcolormesh(timestamps, -depths, signals.T)\n",
        "plt.plot(timestamps, -d_bot, \"b\")\n",
        "plt.plot(timestamps, -d_top, \"c\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transect_pth = \"Survey17/Survey17_GR1_S3W_F\"\n",
        "(\n",
        "    timestamps,\n",
        "    depths,\n",
        "    signals,\n",
        "    d_top,\n",
        "    d_bot,\n",
        ") = echofilter.raw.shardloader.load_transect_from_shards_rel(\n",
        "    transect_pth,\n",
        "    -100,\n",
        "    800,\n",
        "    root_data_dir=ROOT_DATA_DIR,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.pcolormesh(timestamps, -depths, signals.T)\n",
        "plt.plot(timestamps, -d_bot, \"b\")\n",
        "plt.plot(timestamps, -d_top, \"c\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transect_pth = \"Survey17/Survey17_GR1_S3W_F\"\n",
        "(\n",
        "    timestamps,\n",
        "    depths,\n",
        "    signals,\n",
        "    d_top,\n",
        "    d_bot,\n",
        ") = echofilter.raw.shardloader.load_transect_from_shards_rel(\n",
        "    transect_pth,\n",
        "    0,\n",
        "    128,\n",
        "    root_data_dir=ROOT_DATA_DIR,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.pcolormesh(timestamps, -depths, signals.T)\n",
        "plt.plot(timestamps, -d_bot, \"b\")\n",
        "plt.plot(timestamps, -d_top, \"c\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.utils.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransectDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transect_paths,\n",
        "        window_len=128,\n",
        "        crop_depth=70,\n",
        "        num_windows_per_transect=0,\n",
        "        use_dynamic_offsets=True,\n",
        "        transform_pre=None,\n",
        "        transform_post=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TransectDataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transect_paths : list\n",
        "            Absolute paths to transects.\n",
        "        window_len : int\n",
        "            Width (number of timestamps) to load. Default is `128`.\n",
        "        crop_depth : float\n",
        "            Maximum depth to include, in metres. Deeper data will be cropped away.\n",
        "            Default is `70`.\n",
        "        num_windows_per_transect : int\n",
        "            Number of windows to extract for each transect. Start indices for the\n",
        "            windows will be equally spaced across the total width of the transect.\n",
        "            If this is `0`, the number of windows will be inferred automatically\n",
        "            based on `window_len` and the total width of the transect, resulting\n",
        "            in a different number of windows for each transect. Default is `0`.\n",
        "        use_dynamic_offsets : bool\n",
        "            Whether starting indices for each window should be randomly offset.\n",
        "            Set to `True` for training and `False` for testing. Default is `True`.\n",
        "        transform_pre : callable\n",
        "            Operations to perform to the dictionary containing a single sample.\n",
        "            These are performed before generating the masks. Default is `None`.\n",
        "        transform_post : callable\n",
        "            Operations to perform to the dictionary containing a single sample.\n",
        "            These are performed after generating the masks. Default is `None`.\n",
        "        \"\"\"\n",
        "        super(TransectDataset, self).__init__()\n",
        "        self.window_len = window_len\n",
        "        self.crop_depth = crop_depth\n",
        "        self.num_windows = num_windows_per_transect\n",
        "        self.use_dynamic_offsets = use_dynamic_offsets\n",
        "        self.transform_pre = transform_pre\n",
        "        self.transform_post = transform_post\n",
        "\n",
        "        self.datapoints = []\n",
        "\n",
        "        for transect_path in transect_paths:\n",
        "            # Lookup the number of rows in the transect\n",
        "            # Load the sharding metadata\n",
        "            with open(os.path.join(transect_path, \"shard_size.txt\"), \"r\") as f:\n",
        "                n_timestamps, shard_len = f.readline().strip().split(\",\")\n",
        "                n_timestamps = int(n_timestamps)\n",
        "            # Generate an array for window centers within the transect\n",
        "            # - if this is for training, we want to randomise the offsets\n",
        "            # - if this is for validation, we want stable windows\n",
        "            num_windows = self.num_windows\n",
        "            if self.num_windows is None or self.num_windows == 0:\n",
        "                # Load enough windows to include all datapoints\n",
        "                num_windows = int(np.ceil(n_timestamps / self.window_len))\n",
        "            centers = np.linspace(0, n_timestamps, num_windows + 1)[:num_windows]\n",
        "            if len(centers) > 1:\n",
        "                max_dy_offset = centers[1] - centers[0]\n",
        "            else:\n",
        "                max_dy_offset = n_timestamps\n",
        "            if self.use_dynamic_offsets:\n",
        "                centers += np.random.rand() * max_dy_offset\n",
        "            else:\n",
        "                centers += max_dy_offset / 2\n",
        "            centers = np.round(centers)\n",
        "            # Add each (transect, center) to the list for this epoch\n",
        "            for center_idx in centers:\n",
        "                self.datapoints.append((transect_path, int(center_idx)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        transect_pth, center_idx = self.datapoints[index]\n",
        "        # Load data from shards\n",
        "        (\n",
        "            timestamps,\n",
        "            depths,\n",
        "            signals,\n",
        "            d_top,\n",
        "            d_bot,\n",
        "        ) = echofilter.raw.shardloader.load_transect_from_shards_abs(\n",
        "            transect_pth,\n",
        "            center_idx - int(self.window_len / 2),\n",
        "            center_idx - int(self.window_len / 2) + self.window_len,\n",
        "        )\n",
        "        sample = {\n",
        "            \"timestamps\": timestamps,\n",
        "            \"depths\": depths,\n",
        "            \"signals\": signals,\n",
        "            \"d_top\": d_top,\n",
        "            \"d_bot\": d_bot,\n",
        "        }\n",
        "        if self.transform_pre is not None:\n",
        "            sample = self.transform_pre(sample)\n",
        "        # Apply depth crop\n",
        "        depth_crop_mask = sample[\"depths\"] <= self.crop_depth\n",
        "        sample[\"depths\"] = sample[\"depths\"][depth_crop_mask]\n",
        "        sample[\"signals\"] = sample[\"signals\"][:, depth_crop_mask]\n",
        "        # Convert lines to masks\n",
        "        ddepths = np.broadcast_to(sample[\"depths\"], sample[\"signals\"].shape)\n",
        "        mask_top = np.single(ddepths < np.expand_dims(sample[\"d_top\"], -1))\n",
        "        mask_bot = np.single(ddepths > np.expand_dims(sample[\"d_bot\"], -1))\n",
        "        sample[\"mask_top\"] = mask_top\n",
        "        sample[\"mask_bot\"] = mask_bot\n",
        "        sample[\"r_top\"] = sample[\"d_top\"] / abs(\n",
        "            sample[\"depths\"][-1] - sample[\"depths\"][0]\n",
        "        )\n",
        "        sample[\"r_bot\"] = sample[\"d_bot\"] / abs(\n",
        "            sample[\"depths\"][-1] - sample[\"depths\"][0]\n",
        "        )\n",
        "        if self.transform_post is not None:\n",
        "            sample = self.transform_post(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datapoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transect_paths = [\n",
        "    os.path.join(ROOT_DATA_DIR, \"surveyExports_sharded/Survey17/Survey17_GR1_S3W_F\")\n",
        "] * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = TransectDataset(transect_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.datapoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = dataset[0]\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"signals\"])\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_top\"])\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_bot\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample[\"signals\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sample in loader:\n",
        "    print(sample[\"signals\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import skimage.transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"\n",
        "    Rescale the image(s) in a sample to a given size.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_size : tuple or int\n",
        "        Desired output size. If tuple, output is matched to output_size. If int,\n",
        "        output is square.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            output_size = (output_size, output_size)\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        for key in (\"signals\", \"mask_top\", \"mask_bot\"):\n",
        "            if key in sample:\n",
        "                sample[key] = skimage.transform.resize(\n",
        "                    sample[key],\n",
        "                    self.output_size,\n",
        "                    clip=False,\n",
        "                    preserve_range=False,\n",
        "                )\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"\n",
        "    Normalize mean and standard deviation of image.\n",
        "\n",
        "    Note that changes are made inplace.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : float\n",
        "        Expected sample pixel mean.\n",
        "    stdev : float\n",
        "        Expected sample standard deviation of pixel intensities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, stdev):\n",
        "        self.mean = mean\n",
        "        self.stdev = stdev\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        sample[\"signals\"] -= self.mean\n",
        "        sample[\"signals\"] /= self.stdev\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomReflection(object):\n",
        "    \"\"\"\n",
        "    Randomly reflect a sample.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    axis : int, optional\n",
        "        Axis to reflect. Default is 0.\n",
        "    p : float, optional\n",
        "        Probability of reflection. Default is 0.5.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, axis=0, p=0.5):\n",
        "        self.axis = axis\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            # Nothing to do\n",
        "            return sample\n",
        "\n",
        "        # Reflect x co-ordinates\n",
        "        sample[\"timestamps\"] = sample[\"timestamps\"][::-1]\n",
        "\n",
        "        # Reflect data\n",
        "        for key in (\"signals\", \"d_top\", \"d_bot\", \"mask_top\", \"mask_bot\"):\n",
        "            if key in sample:\n",
        "                sample[key] = np.flip(sample[key], self.axis)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomStretchDepth(object):\n",
        "    \"\"\"\n",
        "    Rescale a set of images in a sample to a given size.\n",
        "\n",
        "    Note that this transform doesn't change images, just the `depth`, `d_top`, and `d_bot`.\n",
        "    Note that changes are made inplace.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_factor : float\n",
        "        Maximum stretch factor. A number between `[1, 1 + max_factor]` will be generated,\n",
        "        and the depth will either be divided or multiplied by the generated stretch\n",
        "        factor.\n",
        "    expected_bottom_gap : float\n",
        "        Expected gap between actual ocean floor and target bottom line.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_factor, expected_bottom_gap=1):\n",
        "        self.max_factor = max_factor\n",
        "        self.expected_bottom_gap = expected_bottom_gap\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        factor = random.uniform(1.0, 1.0 + self.max_factor)\n",
        "\n",
        "        if random.random() > 0.5:\n",
        "            factor = 1.0 / factor\n",
        "\n",
        "        sample[\"d_bot\"] += self.expected_bottom_gap\n",
        "        for key in (\"depths\", \"d_top\", \"d_bot\"):\n",
        "            sample[key] *= factor\n",
        "        sample[\"d_bot\"] -= self.expected_bottom_gap\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomCropWidth(object):\n",
        "    \"\"\"\n",
        "    Randomly crop a sample in the width dimension.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_crop_fraction : float\n",
        "        Maximum amount of material to crop away, as a fraction of the total width.\n",
        "        The `crop_fraction` will be sampled uniformly from the range\n",
        "        `[0, max_crop_fraction]`. The crop is always centred.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_crop_fraction):\n",
        "        self.max_crop_fraction = max_crop_fraction\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        width = sample[\"signals\"].shape[0]\n",
        "\n",
        "        crop_fraction = random.uniform(0.0, self.max_crop_fraction)\n",
        "        crop_amount = crop_fraction * width\n",
        "\n",
        "        lft = int(crop_amount / 2)\n",
        "        rgt = lft + width - int(crop_amount)\n",
        "\n",
        "        # Crop data\n",
        "        for key in (\"timestamps\", \"signals\", \"d_top\", \"d_bot\", \"mask_top\", \"mask_bot\"):\n",
        "            if key in sample:\n",
        "                sample[key] = sample[key][lft:rgt]\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ColorJitter(object):\n",
        "    \"\"\"\n",
        "    Randomly change the brightness and contrast of a normalized image.\n",
        "\n",
        "    Note that changes are made inplace.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    brightness : float or tuple of float (min, max)\n",
        "        How much to jitter brightness. `brightness_factor` is chosen uniformly from\n",
        "        `[-brightness, brightness]`\n",
        "        or the given `[min, max]`. `brightness_factor` is then added to the image.\n",
        "    contrast : (float or tuple of float (min, max))\n",
        "        How much to jitter contrast. `contrast_factor` is chosen uniformly from\n",
        "        `[max(0, 1 - contrast), 1 + contrast]`\n",
        "        or the given `[min, max]`. Should be non negative numbers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, brightness=0, contrast=0):\n",
        "        self.brightness = self._check_input(\n",
        "            brightness,\n",
        "            \"brightness\",\n",
        "            center=0,\n",
        "            bound=(float(\"-inf\"), float(\"inf\")),\n",
        "            clip_first_on_zero=False,\n",
        "        )\n",
        "        self.contrast = self._check_input(contrast, \"contrast\")\n",
        "\n",
        "    def _check_input(\n",
        "        self, value, name, center=1, bound=(0, float(\"inf\")), clip_first_on_zero=True\n",
        "    ):\n",
        "        if isinstance(value, (float, int)):\n",
        "            if value < 0:\n",
        "                raise ValueError(\n",
        "                    \"If {} is a single number, it must be non negative.\".format(name)\n",
        "                )\n",
        "            value = [center - value, center + value]\n",
        "            if clip_first_on_zero:\n",
        "                value[0] = max(value[0], 0)\n",
        "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
        "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
        "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                \"{} should be a single number or a list/tuple with length 2.\".format(\n",
        "                    name\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if value[0] == value[1] == center:\n",
        "            value = None\n",
        "        return value\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        init_op = random.randint(0, 1)\n",
        "        for i_op in range(2):\n",
        "            op_num = (init_op + i_op) % 2\n",
        "            if op_num == 0 and self.brightness is not None:\n",
        "                brightness_factor = random.uniform(\n",
        "                    self.brightness[0], self.brightness[1]\n",
        "                )\n",
        "                sample[\"signals\"] += brightness_factor\n",
        "            elif op_num == 1 and self.contrast is not None:\n",
        "                contrast_factor = random.uniform(self.contrast[0], self.contrast[1])\n",
        "                sample[\"signals\"] *= contrast_factor\n",
        "        return sample\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        format_string += \"brightness={0}\".format(self.brightness)\n",
        "        format_string += \", contrast={0})\".format(self.contrast)\n",
        "        format_string += \")\"\n",
        "        return format_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform_pre = torchvision.transforms.Compose(\n",
        "    [\n",
        "        RandomCropWidth(0.5),\n",
        "        RandomStretchDepth(0.5),\n",
        "        RandomReflection(),\n",
        "    ]\n",
        ")\n",
        "train_transform_post = torchvision.transforms.Compose(\n",
        "    [\n",
        "        Rescale((128, 512)),\n",
        "        Normalize(-70, 22),\n",
        "        ColorJitter(0.5, 0.3),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = TransectDataset(\n",
        "    transect_paths,\n",
        "    window_len=192,\n",
        "    crop_depth=70,\n",
        "    num_windows_per_transect=10,\n",
        "    use_dynamic_offsets=True,\n",
        "    transform_pre=train_transform_pre,\n",
        "    transform_post=train_transform_post,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = dataset_train[0]\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.pcolormesh(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"signals\"].shape[0]),\n",
        "    -np.linspace(sample[\"depths\"][0], sample[\"depths\"][-1], sample[\"signals\"].shape[1]),\n",
        "    sample[\"signals\"].T,\n",
        ")\n",
        "plt.plot(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"d_bot\"].shape[0]),\n",
        "    -sample[\"d_bot\"],\n",
        "    \"b\",\n",
        ")\n",
        "plt.plot(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"d_top\"].shape[0]),\n",
        "    -sample[\"d_top\"],\n",
        "    \"c\",\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"signals\"])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_top\"])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_bot\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample[\"r_top\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample[\"r_bot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        Rescale((128, 512)),\n",
        "        Normalize(-70, 22),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset_val = TransectDataset(\n",
        "    transect_paths,\n",
        "    window_len=128,\n",
        "    crop_depth=70,\n",
        "    num_windows_per_transect=20,\n",
        "    use_dynamic_offsets=False,\n",
        "    transform_post=val_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = dataset_val[0]\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.pcolormesh(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"signals\"].shape[0]),\n",
        "    -np.linspace(sample[\"depths\"][0], sample[\"depths\"][-1], sample[\"signals\"].shape[1]),\n",
        "    sample[\"signals\"].T,\n",
        ")\n",
        "plt.plot(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"d_bot\"].shape[0]),\n",
        "    -sample[\"d_bot\"],\n",
        "    \"b\",\n",
        ")\n",
        "plt.plot(\n",
        "    np.linspace(*sample[\"timestamps\"][[0, -1]], sample[\"d_top\"].shape[0]),\n",
        "    -sample[\"d_top\"],\n",
        "    \"c\",\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"signals\"])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_top\"])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(sample[\"mask_bot\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_val.datapoints"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
